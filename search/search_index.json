{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BOLD Best Practice Manifesto \u00b6 BOLD has been set up as a central programme to drive data linking and realise this potential value for the country BOLD Aims \u00b6 BOLD aims to enable better-evidenced, joined-up and more effective cross-government services to support and protect vulnerable adults at specific touchpoints in their interactions with government-funded services, supported by timely linked data and evidence. Aims for this best practice document \u00b6 THIS PART IS STILL BEING WORKED ON (WILL BE UPDATED SOON) * Whilst incredibly powerful, the approaches discussed should not be seen as panacea for all the difficulties of producing analysis. However, implementing even a few of these techniques can drive benefits in auditability, speed, quality, and knowledge transfer. This is a \"live\" document that can be updated when it is decided that something needs to be added so be sure to check it regularly / have it in your bookmarks. Structure of this best practice document \u00b6 Section 1 is about just starting analytical projects using code Section 2 is about not repeating ourselves and using reusable structures called functions Section 3 discusses about the importance of a clear layout structure. Section 4 presents recommended coding standards when coding and collaborating with others Section 5 shows examples of using version control to archive progress / collaborate Section 6 points out different ways of making your analysis reproducible Section 7 talks about testing. An important step in order to ensure/prove the correctness of parts that form our analysis What to look for according to level \u00b6 Beginner level \u00b6 It would be useful to start from the beginning and cover all sections Intermediate level \u00b6 It would be useful to start from the section 3 , \"Clear Layout Structure\" and cover all subsequent sections Advanced level \u00b6 It would be useful to start from the section 6 , \"Reproducible Work\" and cover all subsequent sections How to contribute \u00b6 You can add your suggestions for this document via the Github Issues interface. Please see the Contributing section for further details.","title":"Home"},{"location":"#bold-best-practice-manifesto","text":"BOLD has been set up as a central programme to drive data linking and realise this potential value for the country","title":"BOLD Best Practice Manifesto"},{"location":"#bold-aims","text":"BOLD aims to enable better-evidenced, joined-up and more effective cross-government services to support and protect vulnerable adults at specific touchpoints in their interactions with government-funded services, supported by timely linked data and evidence.","title":"BOLD Aims"},{"location":"#aims-for-this-best-practice-document","text":"THIS PART IS STILL BEING WORKED ON (WILL BE UPDATED SOON) * Whilst incredibly powerful, the approaches discussed should not be seen as panacea for all the difficulties of producing analysis. However, implementing even a few of these techniques can drive benefits in auditability, speed, quality, and knowledge transfer. This is a \"live\" document that can be updated when it is decided that something needs to be added so be sure to check it regularly / have it in your bookmarks.","title":"Aims for this best practice document"},{"location":"#structure-of-this-best-practice-document","text":"Section 1 is about just starting analytical projects using code Section 2 is about not repeating ourselves and using reusable structures called functions Section 3 discusses about the importance of a clear layout structure. Section 4 presents recommended coding standards when coding and collaborating with others Section 5 shows examples of using version control to archive progress / collaborate Section 6 points out different ways of making your analysis reproducible Section 7 talks about testing. An important step in order to ensure/prove the correctness of parts that form our analysis","title":"Structure of this best practice document"},{"location":"#what-to-look-for-according-to-level","text":"","title":"What to look for according to level"},{"location":"#beginner-level","text":"It would be useful to start from the beginning and cover all sections","title":"Beginner level"},{"location":"#intermediate-level","text":"It would be useful to start from the section 3 , \"Clear Layout Structure\" and cover all subsequent sections","title":"Intermediate level"},{"location":"#advanced-level","text":"It would be useful to start from the section 6 , \"Reproducible Work\" and cover all subsequent sections","title":"Advanced level"},{"location":"#how-to-contribute","text":"You can add your suggestions for this document via the Github Issues interface. Please see the Contributing section for further details.","title":"How to contribute"},{"location":"1_adhocscripts/","text":"1 Ad Hoc work \u00b6 Starting point \u00b6 Everyone starts somewhere. Usually every project in the beginning starts by someone trying something out a possible solution to a business problem with perhaps hardcoded minimal inputs and outputs. However at this point code may not be well named or organised. Probably at this point this is not work that will be ready for sharing more widely. Can we do better? \u00b6 There are some ways that even in this initial stage work can be useful as a stepping stone to next steps. An analyst can work in an exploratory mode, investigating processes and building familiarity with the problem domain. Usually work in this step is comprised of single file/ page programs that are in one file also known as scripts Info the focus on this first step should be on learning and reducing uncertainties. Its an important first step on learning how to automate processes via programming and not doing things by hand / manually. Why someone should want to use Jupyter Notebooks? \u00b6 By using technologies such as Jupyter notebooks (in Python/R) an analyst is able to explain how a process works to other people or him/herself after 6 months (!).Jupyter notebooks provide a quick and streamlined way for problem-solvers to prototype code and quickly share it. So in order to get started it is usually very helpful to have some code running in a Jupyter notebook In a way, Jupyter notebooks strike a balance between simple text editors, which are fast to start and simple and easy to manipulate, and IDE 's (an acronym for \"Integrated Development Environment\") which tend to start slower and be feature-rich and complex. Simple text editors typically can only edit code, and cannot run the code. A full IDE can edit code, run the code, debug code, provide syntax highlighting and context help. In the context of problem-solving, Jupyter notebooks are quite handy. Jupyter notebooks open quickly and quickly produce output. Data exploration, data cleaning, and plot building are accomplished in Jupyter notebooks easier and quicker than in a text editor or an IDE. Why someone should not work ONLY with Jupyter Notebooks and work with an IDE \u00b6 Jupyer notebooks are great for telling data stories. But, unless we are doing pure research, our research is the means to an end \u2014 and that is, getting valuable insights from our data stories and data models. A production-grade pipeline needs to be composed out of debuggable, reproducible, easy-to-deploy, high-performance code components that could be orchestrated and scheduled. In its default version, it\u2019s everything adhoc code on Jupyter isn\u2019t. Out of 400 cells, could cell # 273 run even if cell #50 did not run? What if it did run, but on a different data, reading different data? Are cell #200\u2019s results immutable? I.e. can we re-run it and get the same results, or will a re-run fail/return different results? There is no easy way of answering any of those questions while developing on a Jupyter Notebook. You have to rely on your human memory to know which cell could run with/without other cells, which cell is re-runnable, etc. This just doesn\u2019t work. Basically, what most data scientists do when they are unsure about the state of a notebook, is just trigger a complete re-run of the entire notebook, which is a complete waste of time and resources. Notebooks are useful tools for interactive data exploration which is the dominant activity when working on the early phase of a new project or exploring a new technique. But once an approach has been settled on, the focus needs to shift to building a structured codebase around this approach while retaining some ability to experiment. So what is the solution? \u00b6 For R the use of RStudio , a fully fledged IDE (\"Integrated Development Environment\") is recommended. Thankfully that should not be a big change to an analyst as its the main way most people use R these days. For Python VSCode is recommended. Next Steps \u00b6 While just getting something to work in order to prove a point is good as far as running an adhoc program is concerned, there are a few ways of improving the work we can produce. Hopefully you will find some interesting suggestions on how to do this in the following sections. what we want to avoid image courtesy of Piled Higher and Deeper / PHDComics","title":"1 Ad hoc scripts"},{"location":"1_adhocscripts/#1-ad-hoc-work","text":"","title":"1 Ad Hoc work"},{"location":"1_adhocscripts/#starting-point","text":"Everyone starts somewhere. Usually every project in the beginning starts by someone trying something out a possible solution to a business problem with perhaps hardcoded minimal inputs and outputs. However at this point code may not be well named or organised. Probably at this point this is not work that will be ready for sharing more widely.","title":"Starting point"},{"location":"1_adhocscripts/#can-we-do-better","text":"There are some ways that even in this initial stage work can be useful as a stepping stone to next steps. An analyst can work in an exploratory mode, investigating processes and building familiarity with the problem domain. Usually work in this step is comprised of single file/ page programs that are in one file also known as scripts Info the focus on this first step should be on learning and reducing uncertainties. Its an important first step on learning how to automate processes via programming and not doing things by hand / manually.","title":"Can we do better?"},{"location":"1_adhocscripts/#why-someone-should-want-to-use-jupyter-notebooks","text":"By using technologies such as Jupyter notebooks (in Python/R) an analyst is able to explain how a process works to other people or him/herself after 6 months (!).Jupyter notebooks provide a quick and streamlined way for problem-solvers to prototype code and quickly share it. So in order to get started it is usually very helpful to have some code running in a Jupyter notebook In a way, Jupyter notebooks strike a balance between simple text editors, which are fast to start and simple and easy to manipulate, and IDE 's (an acronym for \"Integrated Development Environment\") which tend to start slower and be feature-rich and complex. Simple text editors typically can only edit code, and cannot run the code. A full IDE can edit code, run the code, debug code, provide syntax highlighting and context help. In the context of problem-solving, Jupyter notebooks are quite handy. Jupyter notebooks open quickly and quickly produce output. Data exploration, data cleaning, and plot building are accomplished in Jupyter notebooks easier and quicker than in a text editor or an IDE.","title":"Why someone should want to use Jupyter Notebooks?"},{"location":"1_adhocscripts/#why-someone-should-not-work-only-with-jupyter-notebooks-and-work-with-an-ide","text":"Jupyer notebooks are great for telling data stories. But, unless we are doing pure research, our research is the means to an end \u2014 and that is, getting valuable insights from our data stories and data models. A production-grade pipeline needs to be composed out of debuggable, reproducible, easy-to-deploy, high-performance code components that could be orchestrated and scheduled. In its default version, it\u2019s everything adhoc code on Jupyter isn\u2019t. Out of 400 cells, could cell # 273 run even if cell #50 did not run? What if it did run, but on a different data, reading different data? Are cell #200\u2019s results immutable? I.e. can we re-run it and get the same results, or will a re-run fail/return different results? There is no easy way of answering any of those questions while developing on a Jupyter Notebook. You have to rely on your human memory to know which cell could run with/without other cells, which cell is re-runnable, etc. This just doesn\u2019t work. Basically, what most data scientists do when they are unsure about the state of a notebook, is just trigger a complete re-run of the entire notebook, which is a complete waste of time and resources. Notebooks are useful tools for interactive data exploration which is the dominant activity when working on the early phase of a new project or exploring a new technique. But once an approach has been settled on, the focus needs to shift to building a structured codebase around this approach while retaining some ability to experiment.","title":"Why someone should not work ONLY with Jupyter Notebooks and work with an IDE"},{"location":"1_adhocscripts/#so-what-is-the-solution","text":"For R the use of RStudio , a fully fledged IDE (\"Integrated Development Environment\") is recommended. Thankfully that should not be a big change to an analyst as its the main way most people use R these days. For Python VSCode is recommended.","title":"So what is the solution?"},{"location":"1_adhocscripts/#next-steps","text":"While just getting something to work in order to prove a point is good as far as running an adhoc program is concerned, there are a few ways of improving the work we can produce. Hopefully you will find some interesting suggestions on how to do this in the following sections. what we want to avoid image courtesy of Piled Higher and Deeper / PHDComics","title":"Next Steps"},{"location":"2_reusable_code/","text":"2 Reusable code \u00b6 Motivation \u00b6 Breaking your code down into smaller, more manageable chunks is a sensible way to improve readability and extensibility. Regardless of the language, there are techniques to containerise your code into self-contained parts such as modules or functions. Adding repeating, unnecessary code to a codebase increases the amount of work required to extend and maintain the software in the future . Duplicate code adds to technical debt . Whether the duplication stems from \"Copy Paste\" programming or poor understanding of how to apply abstraction, it decreases the quality of the code. It is hence important to use the DRY Don't Repeat Yourself principle in order to create work that is more managable. Info DRY (Don't Repeat Yourself) principle: The same piece of code should not be repeated over and over again. Functions \u00b6 In the early stages of analysis we often copy and paste code to \u2018make it work\u2019. As this work matures, it is worth taking repetitive code and turning it into functions. Functions allow us to make a piece of logic reusable in a consistent and readable way, A function is simply a way to \u201cchunk\u201d some code that you can use over and over again, rather than writing it out multiple times. Functions enable programmers to break down or decompose a problem into smaller parts, each of which performs a particular task. In this way code becomes easier to reason and to follow through in analysis scripts. It also provides a way of changing code once if the function inner-workings need to change instead of many repeated times. You can find some really helpful pointers on how to create functions in R in the following DASD R Training : writing functions in r Likewise a comprehensive tutorial on how to create functions in Python can ve found here: defining your own python function Referential transparency \u00b6 When writing functions, it\u2019s important to consider how they interact with other parts of your code. As a general rule of thumb, your code should run in the same way if a call to your function was replaced by the value that it would have returned. This is referred to as \u2018referential transparency\u2019. In practice, this means that your functions should not depend-on or affect variables that have not been explicitly fed into them as arguments. For instance, a function should not add columns to a data table that has not been passed as an input to the function. Nor should the action of a function be affected by anything other than arguments that are passed to it. For example, running your function twice with the same inputs should always produce the same results. Modular design of code and seperation of concerns \u00b6 By understanding the mechanism of running code that is located in different files the analyst gains the ability to create modular code that has all related functionality at the same place while achieving seperation of concerns The general idea is that one should avoid co-locating different concerns within the design or code. Separation of concerns tends to be a natural consequence of following the Don't Repeat Yourself (DRY) principle In Python it is well worth understanding the import mechanism in order to be able to use not only external packages but code that is bundled together because it makes sense for it to be co-located. It is worth looking at this tutorial to understand a bit more about how imports in Python can be used. Similarly in R its worth using both the library mechanism in order to load external packages and be able to use package functions into the analysis code together with the source mechanism in which internal code can be also become available for use. The source documentation will be useful.","title":"2 Reusable Code"},{"location":"2_reusable_code/#2-reusable-code","text":"","title":"2 Reusable code"},{"location":"2_reusable_code/#motivation","text":"Breaking your code down into smaller, more manageable chunks is a sensible way to improve readability and extensibility. Regardless of the language, there are techniques to containerise your code into self-contained parts such as modules or functions. Adding repeating, unnecessary code to a codebase increases the amount of work required to extend and maintain the software in the future . Duplicate code adds to technical debt . Whether the duplication stems from \"Copy Paste\" programming or poor understanding of how to apply abstraction, it decreases the quality of the code. It is hence important to use the DRY Don't Repeat Yourself principle in order to create work that is more managable. Info DRY (Don't Repeat Yourself) principle: The same piece of code should not be repeated over and over again.","title":"Motivation"},{"location":"2_reusable_code/#functions","text":"In the early stages of analysis we often copy and paste code to \u2018make it work\u2019. As this work matures, it is worth taking repetitive code and turning it into functions. Functions allow us to make a piece of logic reusable in a consistent and readable way, A function is simply a way to \u201cchunk\u201d some code that you can use over and over again, rather than writing it out multiple times. Functions enable programmers to break down or decompose a problem into smaller parts, each of which performs a particular task. In this way code becomes easier to reason and to follow through in analysis scripts. It also provides a way of changing code once if the function inner-workings need to change instead of many repeated times. You can find some really helpful pointers on how to create functions in R in the following DASD R Training : writing functions in r Likewise a comprehensive tutorial on how to create functions in Python can ve found here: defining your own python function","title":"Functions"},{"location":"2_reusable_code/#referential-transparency","text":"When writing functions, it\u2019s important to consider how they interact with other parts of your code. As a general rule of thumb, your code should run in the same way if a call to your function was replaced by the value that it would have returned. This is referred to as \u2018referential transparency\u2019. In practice, this means that your functions should not depend-on or affect variables that have not been explicitly fed into them as arguments. For instance, a function should not add columns to a data table that has not been passed as an input to the function. Nor should the action of a function be affected by anything other than arguments that are passed to it. For example, running your function twice with the same inputs should always produce the same results.","title":"Referential transparency"},{"location":"2_reusable_code/#modular-design-of-code-and-seperation-of-concerns","text":"By understanding the mechanism of running code that is located in different files the analyst gains the ability to create modular code that has all related functionality at the same place while achieving seperation of concerns The general idea is that one should avoid co-locating different concerns within the design or code. Separation of concerns tends to be a natural consequence of following the Don't Repeat Yourself (DRY) principle In Python it is well worth understanding the import mechanism in order to be able to use not only external packages but code that is bundled together because it makes sense for it to be co-located. It is worth looking at this tutorial to understand a bit more about how imports in Python can be used. Similarly in R its worth using both the library mechanism in order to load external packages and be able to use package functions into the analysis code together with the source mechanism in which internal code can be also become available for use. The source documentation will be useful.","title":"Modular design of code and seperation of concerns"},{"location":"3_clear_layout_structure/","text":"3 Use a clear layout structure \u00b6 Motivation \u00b6 A well-defined, standard project structure means that a newcomer can begin to understand an analysis without digging in to extensive documentation. It also means that they don't necessarily have to read 100% of the code before knowing where to look for very specific things. Well organized code tends to be self-documenting in that the organization itself provides context for your code without much overhead. People will thank you for this because they can: Collaborate more easily with you on this analysis Learn from your analysis about the process and the domain Feel confident in the conclusions at which the analysis arrives use meaningful names image courtesy of Nathan W Pyle Use directory hierarchy \u00b6 Some proposed basic suggestions are: to have a README file at the root of the project where a user can find out more about the project in hand. Perhaps also the set of commands needed to run an analysis script. There should be separate directories for the analysis inputs, code itself & the outputs. This will also help at a later point as it will make data breaches less likely. There needs to be a separate directory for \u2018ad hoc\u2019 exploratory code/notebooks (like the ones discussed in a previous section) so that they don\u2019t interfere in any way with the main code. What to include on a README file? \u00b6 When working on a collaborative or open coding project, it\u2019s good practice to describe an overview of your project in a README file. This allows users or developers to grasp the overall goal of your project. As well as a description of the project, it might include examples using your code or references to other associated projects. This file can be any text type, including .txt, .md, We suggest the following for a good README: Short statement of intent Longer description describing the problem that your project solves and how it solves it Basic installation instructions or link to installation guide Example usage Links to related projects Use meaningful folder/file names \u00b6 A very detailed example of how a project could look (example for a Python project) \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 .gitignore <- files that should not be commited to a repo \u251c\u2500\u2500 README.md <- The top-level README \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 external <- Data from third party sources. \u2502 \u251c\u2500\u2500 interim <- Intermediate data that has been transformed. \u2502 \u251c\u2500\u2500 processed <- The final, canonical data sets for modeling. \u2502 \u2514\u2500\u2500 raw <- The original, immutable data dump. \u2502 \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 models <- Trained and serialized models,model predictions,or model summaries \u2502 \u251c\u2500\u2500 adhocnotebooks <- Jupyter notebooks. Naming convention is a number (for ordering), \u2502 the creator's initials, and a short `-` delimited description, \u2502 e.g. 1.0-jqp-initial-data-exploration`. \u2502 \u251c\u2500\u2500 references <- Data dictionaries, manuals, and all other explanatory materials. \u2502 \u251c\u2500\u2500 reports <- Generated analysis as HTML, PDF, LaTeX, etc. \u2502 \u2514\u2500\u2500 figures <- Generated graphics and figures to be used in reporting \u2502 \u251c\u2500\u2500 src <- Source code for use in this project. \u251c\u2500\u2500 __init__.py <- Makes src a Python module if needed \u2502 \u251c\u2500\u2500 data <- Scripts to download or generate data \u2502 \u2514\u2500\u2500 make_dataset.py \u2502 \u251c\u2500\u2500 features <- Scripts to turn raw data into features for modeling \u2502 \u2514\u2500\u2500 build_features.py \u2502 \u251c\u2500\u2500 models <- Scripts to train models and then use trained models to make \u2502 \u2502 predictions \u2502 \u251c\u2500\u2500 predict_model.py \u2502 \u2514\u2500\u2500 train_model.py \u2502 \u2514\u2500\u2500 visualization <- Scripts to create exploratory and results oriented visualizations \u2514\u2500\u2500 visualize.py Proposed Python helper \u00b6 There is a project that creates a folder structure for python projects called cookiecutter A suggested BOLD solution is that we can create a BOLD cookiecutter template that every analyst can use by typing cookiecutter BOLD-Template and then can provide the agreed upon directories of a starting slate We can agree on what will be needed in a discusion via the Github Issues interface Proposed R helper \u00b6 Similarly for R: projecttemplate According to its documentation ProjectTemplate is a system for automating the thoughtless parts of a data analysis project: Besides a full project layout it offers that is created by default, it is also possible to create a minimal project structure that only contains the mandatory directories and files. This can be created using create.project(template='minimal') , and results in the following structure: \u251c\u2500\u2500 project \u251c\u2500\u2500 cache \u251c\u2500\u2500 README.md <- The top-level README \u251c\u2500\u2500 data \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 global.dcf \u251c\u2500\u2500 munge \u2502 \u2514\u2500\u2500 01-A.R \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 eda.R It is recommended to start with this simple template and to change it then to taste.","title":"3 Clear layout structure"},{"location":"3_clear_layout_structure/#3-use-a-clear-layout-structure","text":"","title":"3 Use a clear layout structure"},{"location":"3_clear_layout_structure/#motivation","text":"A well-defined, standard project structure means that a newcomer can begin to understand an analysis without digging in to extensive documentation. It also means that they don't necessarily have to read 100% of the code before knowing where to look for very specific things. Well organized code tends to be self-documenting in that the organization itself provides context for your code without much overhead. People will thank you for this because they can: Collaborate more easily with you on this analysis Learn from your analysis about the process and the domain Feel confident in the conclusions at which the analysis arrives use meaningful names image courtesy of Nathan W Pyle","title":"Motivation"},{"location":"3_clear_layout_structure/#use-directory-hierarchy","text":"Some proposed basic suggestions are: to have a README file at the root of the project where a user can find out more about the project in hand. Perhaps also the set of commands needed to run an analysis script. There should be separate directories for the analysis inputs, code itself & the outputs. This will also help at a later point as it will make data breaches less likely. There needs to be a separate directory for \u2018ad hoc\u2019 exploratory code/notebooks (like the ones discussed in a previous section) so that they don\u2019t interfere in any way with the main code.","title":"Use directory hierarchy"},{"location":"3_clear_layout_structure/#what-to-include-on-a-readme-file","text":"When working on a collaborative or open coding project, it\u2019s good practice to describe an overview of your project in a README file. This allows users or developers to grasp the overall goal of your project. As well as a description of the project, it might include examples using your code or references to other associated projects. This file can be any text type, including .txt, .md, We suggest the following for a good README: Short statement of intent Longer description describing the problem that your project solves and how it solves it Basic installation instructions or link to installation guide Example usage Links to related projects","title":"What to include on a README file?"},{"location":"3_clear_layout_structure/#use-meaningful-folderfile-names","text":"A very detailed example of how a project could look (example for a Python project) \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 .gitignore <- files that should not be commited to a repo \u251c\u2500\u2500 README.md <- The top-level README \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 external <- Data from third party sources. \u2502 \u251c\u2500\u2500 interim <- Intermediate data that has been transformed. \u2502 \u251c\u2500\u2500 processed <- The final, canonical data sets for modeling. \u2502 \u2514\u2500\u2500 raw <- The original, immutable data dump. \u2502 \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 models <- Trained and serialized models,model predictions,or model summaries \u2502 \u251c\u2500\u2500 adhocnotebooks <- Jupyter notebooks. Naming convention is a number (for ordering), \u2502 the creator's initials, and a short `-` delimited description, \u2502 e.g. 1.0-jqp-initial-data-exploration`. \u2502 \u251c\u2500\u2500 references <- Data dictionaries, manuals, and all other explanatory materials. \u2502 \u251c\u2500\u2500 reports <- Generated analysis as HTML, PDF, LaTeX, etc. \u2502 \u2514\u2500\u2500 figures <- Generated graphics and figures to be used in reporting \u2502 \u251c\u2500\u2500 src <- Source code for use in this project. \u251c\u2500\u2500 __init__.py <- Makes src a Python module if needed \u2502 \u251c\u2500\u2500 data <- Scripts to download or generate data \u2502 \u2514\u2500\u2500 make_dataset.py \u2502 \u251c\u2500\u2500 features <- Scripts to turn raw data into features for modeling \u2502 \u2514\u2500\u2500 build_features.py \u2502 \u251c\u2500\u2500 models <- Scripts to train models and then use trained models to make \u2502 \u2502 predictions \u2502 \u251c\u2500\u2500 predict_model.py \u2502 \u2514\u2500\u2500 train_model.py \u2502 \u2514\u2500\u2500 visualization <- Scripts to create exploratory and results oriented visualizations \u2514\u2500\u2500 visualize.py","title":"Use meaningful folder/file names"},{"location":"3_clear_layout_structure/#proposed-python-helper","text":"There is a project that creates a folder structure for python projects called cookiecutter A suggested BOLD solution is that we can create a BOLD cookiecutter template that every analyst can use by typing cookiecutter BOLD-Template and then can provide the agreed upon directories of a starting slate We can agree on what will be needed in a discusion via the Github Issues interface","title":"Proposed Python helper"},{"location":"3_clear_layout_structure/#proposed-r-helper","text":"Similarly for R: projecttemplate According to its documentation ProjectTemplate is a system for automating the thoughtless parts of a data analysis project: Besides a full project layout it offers that is created by default, it is also possible to create a minimal project structure that only contains the mandatory directories and files. This can be created using create.project(template='minimal') , and results in the following structure: \u251c\u2500\u2500 project \u251c\u2500\u2500 cache \u251c\u2500\u2500 README.md <- The top-level README \u251c\u2500\u2500 data \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 global.dcf \u251c\u2500\u2500 munge \u2502 \u2514\u2500\u2500 01-A.R \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 eda.R It is recommended to start with this simple template and to change it then to taste.","title":"Proposed R helper"},{"location":"4_coding_standards/","text":"4 Coding standards \u00b6 Motivation \u00b6 Quote Code is read more often than it is written. -- Guido van Rossum (creator of Python) When writing code, we should expect that at some point someone else will need to understand, use and adapt it. This might be yourself in six months time. As such, it is important to empathise with these potential users and write code that is tidy, understandable and does not add unnecessary complexity. Common barriers to writing readable code include: documentation that is hard to understand or absent, walls of repetitive code that is hard to absorb, or over-complicated code where a simpler solution could be used. Avoiding these issues is essential to make sure that your analysis is reproducible, auditable and assured. Therefore it is our professional responsibility to avoid putting such barriers in place whenever possible. Sensible defaults \u00b6 This section sets out sensible defaults which you are recommended to follow. They are not strict rules, but are recommended points that you are encouraged to know about Python Be aware of the Python Style guide also known as PEP8 .You can use the black formatter so that your code can follow PEP8 automatically. Use Python 3 instead of Python 2. Use versions of Python that have security support and will be supported for a while. At the moment of writing 3.8 is the recommended version but this can change with time. Consult this page to see when security support ends for different versions Use pandas for data analysis on smaller datasets. Pyspark for bigger datasets R Follow the Tidyverse Style Guide Default to packages from the Tidyverse, because they have been carefully designed to work together effectively as part of a modern data analysis workflow. More info can be found here: R for Data Science by Hadley Wickham . Naming conventions \u00b6 Use meaningful names - well-named functions and variables can remove the need for a comment and make life a little easier for other readers, including your future self. Avoid meaningless names like \u2018obj\u2019 / \u2018result\u2019 / \u2018foo\u2019. Don\u2019t be cute or jokey when naming things. Use single-letter variables only where the letter represents a well-known mathematical property (e.g. e = mc^2), or where their meaning is otherwise clear. Clear and concise code \u00b6 Choose clarity over cleverness - use advanced language tricks with care. Less code is usually better - but not at the expense of clarity. Comments / documentation \u00b6 Comments are lines of text in source code files that typically aren\u2019t executed as part of the program. They are small notes or annotations written by those working on the code. Often, they provide context or explain the reasoning behind implementation decisions. Comments are essential to help those working on the code in the future to understand any non-obvious details around how and why the code has been written in a particular way. Linters \u00b6 A linter is a tool that analyses code to check for programmatic and stylistic errors. You should apply a linter to review your code formatting, which will mean your coding style will be consistent across projects and make it easier for others to understand. In RStudio, the keyboard shortcut \u2018ctrl+shift+A\u2019 will reformat your code and automate some of the process of passing the linter. If you apply the linter as you work, rather than at the end, you will find it much easier to write code that passes the linter first time. For R use Lintr and follow the Tidyverse Style Guide . For Python, use black and follow PEP8 . If possible, set up your linters to run automatically on all pull requests, using Github Actions. Logging \u00b6 Misuse or failure in the code produces informative error messages. Errors will occur, so write your error messages in a way that they offer useful information to end users and people working on the code. Code configuration is recorded when the code is run. Pipeline route is recorded if decisions are made in code. That can help with possible debugging . Encoding and CSVs \u00b6 Use unicode for character encoding. This means you should convert inputs that include non-ASCII characters to unicode as early as possible in your data processing workflow. If you are outputting to text files, these should be encoded in utf-8 .","title":"4 Coding standards"},{"location":"4_coding_standards/#4-coding-standards","text":"","title":"4 Coding standards"},{"location":"4_coding_standards/#motivation","text":"Quote Code is read more often than it is written. -- Guido van Rossum (creator of Python) When writing code, we should expect that at some point someone else will need to understand, use and adapt it. This might be yourself in six months time. As such, it is important to empathise with these potential users and write code that is tidy, understandable and does not add unnecessary complexity. Common barriers to writing readable code include: documentation that is hard to understand or absent, walls of repetitive code that is hard to absorb, or over-complicated code where a simpler solution could be used. Avoiding these issues is essential to make sure that your analysis is reproducible, auditable and assured. Therefore it is our professional responsibility to avoid putting such barriers in place whenever possible.","title":"Motivation"},{"location":"4_coding_standards/#sensible-defaults","text":"This section sets out sensible defaults which you are recommended to follow. They are not strict rules, but are recommended points that you are encouraged to know about Python Be aware of the Python Style guide also known as PEP8 .You can use the black formatter so that your code can follow PEP8 automatically. Use Python 3 instead of Python 2. Use versions of Python that have security support and will be supported for a while. At the moment of writing 3.8 is the recommended version but this can change with time. Consult this page to see when security support ends for different versions Use pandas for data analysis on smaller datasets. Pyspark for bigger datasets R Follow the Tidyverse Style Guide Default to packages from the Tidyverse, because they have been carefully designed to work together effectively as part of a modern data analysis workflow. More info can be found here: R for Data Science by Hadley Wickham .","title":"Sensible defaults"},{"location":"4_coding_standards/#naming-conventions","text":"Use meaningful names - well-named functions and variables can remove the need for a comment and make life a little easier for other readers, including your future self. Avoid meaningless names like \u2018obj\u2019 / \u2018result\u2019 / \u2018foo\u2019. Don\u2019t be cute or jokey when naming things. Use single-letter variables only where the letter represents a well-known mathematical property (e.g. e = mc^2), or where their meaning is otherwise clear.","title":"Naming conventions"},{"location":"4_coding_standards/#clear-and-concise-code","text":"Choose clarity over cleverness - use advanced language tricks with care. Less code is usually better - but not at the expense of clarity.","title":"Clear and concise code"},{"location":"4_coding_standards/#comments-documentation","text":"Comments are lines of text in source code files that typically aren\u2019t executed as part of the program. They are small notes or annotations written by those working on the code. Often, they provide context or explain the reasoning behind implementation decisions. Comments are essential to help those working on the code in the future to understand any non-obvious details around how and why the code has been written in a particular way.","title":"Comments / documentation"},{"location":"4_coding_standards/#linters","text":"A linter is a tool that analyses code to check for programmatic and stylistic errors. You should apply a linter to review your code formatting, which will mean your coding style will be consistent across projects and make it easier for others to understand. In RStudio, the keyboard shortcut \u2018ctrl+shift+A\u2019 will reformat your code and automate some of the process of passing the linter. If you apply the linter as you work, rather than at the end, you will find it much easier to write code that passes the linter first time. For R use Lintr and follow the Tidyverse Style Guide . For Python, use black and follow PEP8 . If possible, set up your linters to run automatically on all pull requests, using Github Actions.","title":"Linters"},{"location":"4_coding_standards/#logging","text":"Misuse or failure in the code produces informative error messages. Errors will occur, so write your error messages in a way that they offer useful information to end users and people working on the code. Code configuration is recorded when the code is run. Pipeline route is recorded if decisions are made in code. That can help with possible debugging .","title":"Logging"},{"location":"4_coding_standards/#encoding-and-csvs","text":"Use unicode for character encoding. This means you should convert inputs that include non-ASCII characters to unicode as early as possible in your data processing workflow. If you are outputting to text files, these should be encoded in utf-8 .","title":"Encoding and CSVs"},{"location":"5_version_control/","text":"5 Version control \u00b6 Motivation \u00b6 Version control helps us understand what changes we made in the past or why we did a specific analysis in the way we did it, even weeks or months later. With the help of comments and commit messages, each version can explain what changes it contains compared to the previous versions. This is helpful in order to be able to share our analysis and make it auditable or reproducible - which is good scientific practice. A version control system neatly stores a history of changes and who made them, so while it is still easy to access them, your working directory is not cluttered by the debris of previous versions that are necessary to keep just in case. Similarly, with version control, there is no need to leave chunks of code commented should you ever need to come back to an old version again. Finally, version control is invaluable for collaborative projects where different people work on the same code simultaneously and build on each other\u2019s work. It allows the changes made by different people to be tracked and can automatically combine people\u2019s work while saving a great deal of painstaking effort to do so manually. Using version control for your research project means that your work is totally transparent, and because all your actions are recorded, it enables others to reproduce your studies. Moreover, version control hosting services such as GitHub provide a way to communicate and collaborate in a more structured way, such as in pull requests, code reviews, and issues. In BOLD we would like to encourage sharing the code used to produce the data analysis we produce as this enhances accessibility and reuse by others. In this way other analysts can: benefit from your work and build on it learn from your experiences find uses for your code which you had not thought of! Difference between Git and Github. \u00b6 If you are new to Git and Github, it is worth clarifying the difference between Git and Github. Git is the software that looks after the version control of code, whereas Github is the website on which you publish and share your version controlled code. In practice this means you use Git to track versions of your code, and then submit those changes to Github. Can I only use the Github web interface? \u00b6 One of the attributes of git is that usually used through a CLI (Command Line Interface) on a console window. Not all people are comfortable with the command line or sometimes the computers they work on dont allow this (!) If its not possible to use the command line to use git it is very much worth using Github via its web interface. Having saved work at specific poinst in time is always useful. Github also offers some very helpful tools for project managment in a repository through the web interface. Some of them are: the GitHub Issues tracker that can be used in order to track work, feedback, tasks, or bugs. Here is how you can easily create an issue in a repository. Github Discussions : A place where ideas can be discussed that perhaps can then become tasks to do. Here is a quickstart guide on Github Discussions So how you can have version control with Git/Github? . \u00b6 You can Create files - these may contain text, code or both. Work on these files, by changing, deleting or adding new content. Create a snapshot of the file status (also known as version) at this time. This process of creating a snapshot is described as a \u201ccommit\u201d. As you keep saving your work by adding changes, you make more and more snapshots. You can think of these as saving versions of these files while documenting their history. If you need to go back to a previous version of a file because of a mistake, or if you changed your mind about a previous update, you can access the file in your preferred version, or return your entire project to a past state. Non-Linear Development of Your Project with \u201cBranches\u201d \u00b6 So you have your project and you want to add something new or try something out before reflecting the changes in the main project folder. To add something new, you can continue editing your files and save them with the proposed changes. Suppose you want to try something without reflecting the changes in the central repository. In that case, you can use the \u201cbranching\u201d feature . A branch creates a local copy of the main repository where you can work and try new changes. Any work you do on your branch will not be reflected on your main project (referred to as your main branch) so it remains secure and error-free. At the same time, you can test your ideas and troubleshoot in a local branch. When you are happy with the new changes, you can introduce them to the main project. The merge feature in Git allows the independent lines of development in a local branch to get integrated into the main branch. Basic Git Workflow \u00b6 example git workflow example git command sequence git pull #for the latest in the repo you want git checkout -b MYNEWBRANCH #or any other name to start your own # code writing here git add FILES git commit -m \"commit message here\" # more code writing here git add FILES git commit -m \"commit message here\" # even more code writing git add FILES git commit -m \"commit message here\" git push -u origin MYNEWBRANCH # got to github repo at branch MYNEWBRANCH # PULL REQUEST from Github Interface git workflow flowchart graph TB A[Create/initialise a repository] --> B[Add a file to your repo with git add ]; B--> C[commit/save the changes with git commit ]; C--> D[Create a branch / version with git checkout]; D--> E[make a change]; E--> F[commit the change]; F-->G{Finished?}; G -->|No| E; G ---->|Yes| H[Push to branch] H ---> FF{Finished?} FF --> |Yes| I; FF --> |No| E; I[Open a pull request] --> J[Merge your branch to the main branchh] Git recommendations \u00b6 Commit little and often You should commit often and with small changes. One benefit of doing this is that you can identify possible bugs more easily because fewer things are changing at the same time. Make your project version controlled by initialising a Git repository in its directory using git init Add and commit all your files to the repository using git add . then git commit -m \"my informative message here Commit messages should be meaningful and informative In case you want to make changes in specific files only to be committed with git add <FILE> then git commit -m \"my informative message here\" Each commit preferably should make one simple change. Commit often as it makes it easier to be able to return to specific points in your project if something goes wrong. More commits more savepoints ! Develop new features of your project on their own branches, which you can create via git checkout -b <MYNEWBRANCH> and switch between via git checkout < MYNEWBRANCH > Make sure branches have informative names too. Make sure the main branch is kept clean. Only release level code should be here. Make sure each branch has a single purpose and only changes related to that purpose are made on it. Once work on a branch is complete, it is recommended that a Pull Request is sent for review Code review \u00b6 Code review provides additional assurance that code logic is correct, as well as providing feedback on code and problem structuring. For smaller projects, the review only needs to be a simple read-through and sanity check. Code reviews should be initiated through the creation of a pull request. The review should typically involve the reviewer pulling the code to their local machine, testing it, and leaving comments in the pull request. Remember that it\u2019s always easier (for both you and your reviewers) if you commit and push your changes regularly. You should merge branches into the master regularly so that reviewers review little and often, rather than attempting to review your entire codebase all at once. Git turorials and interesting recommended sources of information \u00b6 github quickstart tutorial from Github documentation Learn git with an interactive git simulator . Chapter 1/2/3 of the \"Introduction Sequence\" recommended for beginners. After that feel free to discover other less essential but still useful information on how to use git. Did things go horribly wrong while using git? Dangitgit to the rescue. Want to see if you understand branches well? Here are some questions to test your understanding a book for git called Pro Git that is free explore what you can do with git and get the command you need with gitexplorer","title":"5 Version Control"},{"location":"5_version_control/#5-version-control","text":"","title":"5 Version control"},{"location":"5_version_control/#motivation","text":"Version control helps us understand what changes we made in the past or why we did a specific analysis in the way we did it, even weeks or months later. With the help of comments and commit messages, each version can explain what changes it contains compared to the previous versions. This is helpful in order to be able to share our analysis and make it auditable or reproducible - which is good scientific practice. A version control system neatly stores a history of changes and who made them, so while it is still easy to access them, your working directory is not cluttered by the debris of previous versions that are necessary to keep just in case. Similarly, with version control, there is no need to leave chunks of code commented should you ever need to come back to an old version again. Finally, version control is invaluable for collaborative projects where different people work on the same code simultaneously and build on each other\u2019s work. It allows the changes made by different people to be tracked and can automatically combine people\u2019s work while saving a great deal of painstaking effort to do so manually. Using version control for your research project means that your work is totally transparent, and because all your actions are recorded, it enables others to reproduce your studies. Moreover, version control hosting services such as GitHub provide a way to communicate and collaborate in a more structured way, such as in pull requests, code reviews, and issues. In BOLD we would like to encourage sharing the code used to produce the data analysis we produce as this enhances accessibility and reuse by others. In this way other analysts can: benefit from your work and build on it learn from your experiences find uses for your code which you had not thought of!","title":"Motivation"},{"location":"5_version_control/#difference-between-git-and-github","text":"If you are new to Git and Github, it is worth clarifying the difference between Git and Github. Git is the software that looks after the version control of code, whereas Github is the website on which you publish and share your version controlled code. In practice this means you use Git to track versions of your code, and then submit those changes to Github.","title":"Difference between Git and Github."},{"location":"5_version_control/#can-i-only-use-the-github-web-interface","text":"One of the attributes of git is that usually used through a CLI (Command Line Interface) on a console window. Not all people are comfortable with the command line or sometimes the computers they work on dont allow this (!) If its not possible to use the command line to use git it is very much worth using Github via its web interface. Having saved work at specific poinst in time is always useful. Github also offers some very helpful tools for project managment in a repository through the web interface. Some of them are: the GitHub Issues tracker that can be used in order to track work, feedback, tasks, or bugs. Here is how you can easily create an issue in a repository. Github Discussions : A place where ideas can be discussed that perhaps can then become tasks to do. Here is a quickstart guide on Github Discussions","title":"Can I only use the Github web interface?"},{"location":"5_version_control/#so-how-you-can-have-version-control-with-gitgithub","text":"You can Create files - these may contain text, code or both. Work on these files, by changing, deleting or adding new content. Create a snapshot of the file status (also known as version) at this time. This process of creating a snapshot is described as a \u201ccommit\u201d. As you keep saving your work by adding changes, you make more and more snapshots. You can think of these as saving versions of these files while documenting their history. If you need to go back to a previous version of a file because of a mistake, or if you changed your mind about a previous update, you can access the file in your preferred version, or return your entire project to a past state.","title":"So how you can have version control with Git/Github? ."},{"location":"5_version_control/#non-linear-development-of-your-project-with-branches","text":"So you have your project and you want to add something new or try something out before reflecting the changes in the main project folder. To add something new, you can continue editing your files and save them with the proposed changes. Suppose you want to try something without reflecting the changes in the central repository. In that case, you can use the \u201cbranching\u201d feature . A branch creates a local copy of the main repository where you can work and try new changes. Any work you do on your branch will not be reflected on your main project (referred to as your main branch) so it remains secure and error-free. At the same time, you can test your ideas and troubleshoot in a local branch. When you are happy with the new changes, you can introduce them to the main project. The merge feature in Git allows the independent lines of development in a local branch to get integrated into the main branch.","title":"Non-Linear Development of Your Project with \u201cBranches\u201d"},{"location":"5_version_control/#basic-git-workflow","text":"example git workflow example git command sequence git pull #for the latest in the repo you want git checkout -b MYNEWBRANCH #or any other name to start your own # code writing here git add FILES git commit -m \"commit message here\" # more code writing here git add FILES git commit -m \"commit message here\" # even more code writing git add FILES git commit -m \"commit message here\" git push -u origin MYNEWBRANCH # got to github repo at branch MYNEWBRANCH # PULL REQUEST from Github Interface git workflow flowchart graph TB A[Create/initialise a repository] --> B[Add a file to your repo with git add ]; B--> C[commit/save the changes with git commit ]; C--> D[Create a branch / version with git checkout]; D--> E[make a change]; E--> F[commit the change]; F-->G{Finished?}; G -->|No| E; G ---->|Yes| H[Push to branch] H ---> FF{Finished?} FF --> |Yes| I; FF --> |No| E; I[Open a pull request] --> J[Merge your branch to the main branchh]","title":"Basic Git Workflow"},{"location":"5_version_control/#git-recommendations","text":"Commit little and often You should commit often and with small changes. One benefit of doing this is that you can identify possible bugs more easily because fewer things are changing at the same time. Make your project version controlled by initialising a Git repository in its directory using git init Add and commit all your files to the repository using git add . then git commit -m \"my informative message here Commit messages should be meaningful and informative In case you want to make changes in specific files only to be committed with git add <FILE> then git commit -m \"my informative message here\" Each commit preferably should make one simple change. Commit often as it makes it easier to be able to return to specific points in your project if something goes wrong. More commits more savepoints ! Develop new features of your project on their own branches, which you can create via git checkout -b <MYNEWBRANCH> and switch between via git checkout < MYNEWBRANCH > Make sure branches have informative names too. Make sure the main branch is kept clean. Only release level code should be here. Make sure each branch has a single purpose and only changes related to that purpose are made on it. Once work on a branch is complete, it is recommended that a Pull Request is sent for review","title":"Git recommendations"},{"location":"5_version_control/#code-review","text":"Code review provides additional assurance that code logic is correct, as well as providing feedback on code and problem structuring. For smaller projects, the review only needs to be a simple read-through and sanity check. Code reviews should be initiated through the creation of a pull request. The review should typically involve the reviewer pulling the code to their local machine, testing it, and leaving comments in the pull request. Remember that it\u2019s always easier (for both you and your reviewers) if you commit and push your changes regularly. You should merge branches into the master regularly so that reviewers review little and often, rather than attempting to review your entire codebase all at once.","title":"Code review"},{"location":"5_version_control/#git-turorials-and-interesting-recommended-sources-of-information","text":"github quickstart tutorial from Github documentation Learn git with an interactive git simulator . Chapter 1/2/3 of the \"Introduction Sequence\" recommended for beginners. After that feel free to discover other less essential but still useful information on how to use git. Did things go horribly wrong while using git? Dangitgit to the rescue. Want to see if you understand branches well? Here are some questions to test your understanding a book for git called Pro Git that is free explore what you can do with git and get the command you need with gitexplorer","title":"Git turorials and interesting recommended sources of information"},{"location":"6_reproducible/","text":"6 Reproducible work \u00b6 Motivation and Background \u00b6 When analysts employ transparency in their research - in other words, when they properly document and share the data and processes associated with their analyses - the broader community is able to save valuable time when reproducing or building upon published results. Often, data or code from prior projects will be re-used by new analysts to verify old findings or develop new analyses. By following guidelines for reproducibility, we can easily communicate our work with different stakeholders. This aspect of reproducibility increases the usefulness of our work by enabling others to also easily build on our results, and re-use our materials. This ensures the continuity of a research idea and can even find fresh applications in other contexts. Another gain from reproducibility is knowledge transfer. Many times we will find an analyst that had performed an analysis at some point has moved to a new role.By having work organised with reproducibility in mind this becomes less of an issue. It is very helpful for new joiners as well as reproducible work offers what is known as paradata : an information trail that can be quite helpful when trying to understand a new codebase. Info Reproducible: A result is reproducible when the same analysis steps performed on the same dataset consistently produces the same answer. An short article in Science that can be found in this link is a very good summary of the benefits of reproducibility. Dependencies \u00b6 Dependencies are all of the software components required by your project in order for it to work as intended and avoid runtime errors. One of the problems with working with open source software is that it is quite easy to fall into a trap called \u2018dependency hell\u2019. Essentially, this occurs when the software we write depends on open source packages, which depend on other open source packages, which can depend on other packages, and on, and on. All these packages may be written by many different people, and are updated at vastly different timescales. If we fail to take account of this, then we are likely to fail at the first hurdle of reproducibility, and we may find that in a year\u2019s time we are no longer able to reproduce the work that we previously did - or at least not without a lot of trouble. There are several ways we might get on top of this problem and we in this way we are going to try to explain some ways of dealing with this. Python After installing the following packages pip3 install pipreqs pip3 install pip-tools Use the following code to build a deterministic requirements.txt in the root folder of your project. That will find all requirements needed for your project to run which will be useful at a later point when dependency managment tools are going to be used. pipreqs --savepath = requirements.in && pip-compile R By typing sessionInfo () on your console you can find information about your R script's dependencies. This information is going to be really useful at a later point Dependency managment \u00b6 As seen above when working on a code project, one of the key concerns is dependency management . This usually means dealing with the specific packages/libraries and specific package/library versions needed for a program to run sucessfully This can be done effectively with dependency management tools: Python A tried and tested solution is using conda for this as it also installs libraries in binary format for particular machines (Windows/OSX/Linux). This is quite useful as a lot of times many packages use what is known as bootstrapping where they are compiled from source code to executable packages that can be then imported on the analyst\u2019s computer. Something that is can usually become a problem is that most computers analysts use are locked down as far as access and writing permissions are concerned or they might not include the right C++ compiler. Using conda to install the packages needed in already compiled binary format saves a lot of headaches as far as reproducibility is concerned. R Using a dependency managment tool like Renv is recommended for R. Data is immutable \u00b6 Don't ever edit your raw data, especially not manually, and especially not in Excel. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable. The code you write should move the raw data through a pipeline to your final analysis. You shouldn't have to run all of the steps every time you want to make a new figure Data and code lineage \u00b6 Another matter that can help with reproducibility and pays dividends to be aware of is code and data provenance and lineage. Code lineage \u00b6 Different code can understandably give different results. One way to help navigate this is by having the output of your analysis to also include the git hash of the code that produced it. Python import subprocess def get_git_revision_hash (): return subprocess . check_output ([ 'git' , 'rev-parse' , 'HEAD' ]) def get_git_revision_short_hash (): return subprocess . check_output ([ 'git' , 'rev-parse' , '--short' , 'HEAD' ]) R library ( git2r ) repo <- repository ( \".\" ) print ( repository_head ( repo )) Data lineage \u00b6 There is also a need to do the same for data: Analysis that starts from a different point might have a different result. The solution is to include a hash of the data or any other way to help identifying the starting point of an analysis. MD5 or SH256 can help here example in command prompt: Windows certutil -hashfile <file> MD5 OSX/Linux md5 <file> Keep data out of version control \u00b6 You really don't want to leak your data on a public repo on Github. The way to do this is to learn use .gitignore . In this way data files should never get committed into the version control repository. You can see below hypothetical .gitignore examples for Python and R Python # keep data files in directory DATADIR out of the repo DATADIR/ # keep csv files everywhere out of the repo *.csv # Byte-compiled / optimized / DLL files __pycache__/ *.py [ cod ] * $py .class # C extensions *.so # Distribution / packaging .Python build/ develop-eggs/ R # keep data files in directory DATADIR out of the repo DATADIR/ # keep csv files everywhere out of the repo *.csv # History files .Rhistory .Rapp.history # Session Data files .RData .RDataTmp # User-specific files .Ruserdata # Example code in package build process *-Ex.R # Output files from R CMD build /*.tar.gz # Output files from R CMD check /*.Rcheck/ # RStudio files .Rproj.user/ Keep secrets and configuration out of version control \u00b6 You also don't want to leak your AWS secret key or database username and password on Github. Here's one way to do this: Store your secrets and config variables in a special file (for example called .env) in the root folder Add a line in .gitignore with the name of this special file. In this way this file will never get committed into the version control repository. BOLD project .gitignore template \u00b6 A preliminary .gitignore template has been created and is available here . You can start with it and then edit it further to add other files/directories you want to make sure that they are not commited in a code repo. This can also be included in the cookiekcutter template mentioned in the clear layout structure document pre-commit git hooks \u00b6 Git hooks are scripts that Git executes before or after events such as: commit, push, and receive. Git hooks are a built-in feature - no need to download anything. Git hooks are run locally. These hook scripts are only limited by a person's imagination. Some example hook scripts include: pre-commit: Check the commit message for spelling errors. pre-receive: Enforce project coding standards. post-commit: Email/SMS team members of a new commit. post-receive: Push the code to production. We can use pre-commit git hooks by using pre-commit framework.The pre-commit framework bills itself as \"A framework for managing and maintaining multi-language pre-commit hooks.\" Under the hood, it runs on python, but you can use the framework on any project, regardless of your project's primary language. You can install this by typing pip install pre-commit on your console window Once installed, you're going to add a pre-commit configuration file to your project root named .pre-commit-config.yaml . In that config file, you will need to specify which scripts pre-commit will run when your pre-commit hook is triggered by a git commit command. There are plenty of ready made precommit hooks available here . We will use some of them now to tailor something useful for BOLD. This first example is suitable for Python code bases. A similar example with R will be provided soon. We will add certain hooks to standardise the format of the code. We will also use black to make our code fully PEP8 style guide compliant automatically. The configuration file .pre-commit-config.yaml will look something like this : repos : - repo : https://github.com/pre-commit/pre-commit-hooks rev : v3.2.0 hooks : - id : trailing-whitespace - id : end-of-file-fixer - id : check-added-large-files args : [ '--maxkb=5000' ] description : do not upload files larger than 5000kb / 5MB to avoid data being uploaded - id : mixed-line-ending args : [ '--fix=lf' ] description : Forces to replace line ending by the UNIX 'lf' character. - repo : https://github.com/psf/black rev : 20.8b1 hooks : - id : black After creating .pre-commit-config.yaml we can install it by typing pre-commit install into the root folder where the config file is. If everything has been done correctly a message saying pre-commit installed at .git/hooks/pre-commit will appear.","title":"6 Reproducible work"},{"location":"6_reproducible/#6-reproducible-work","text":"","title":"6 Reproducible work"},{"location":"6_reproducible/#motivation-and-background","text":"When analysts employ transparency in their research - in other words, when they properly document and share the data and processes associated with their analyses - the broader community is able to save valuable time when reproducing or building upon published results. Often, data or code from prior projects will be re-used by new analysts to verify old findings or develop new analyses. By following guidelines for reproducibility, we can easily communicate our work with different stakeholders. This aspect of reproducibility increases the usefulness of our work by enabling others to also easily build on our results, and re-use our materials. This ensures the continuity of a research idea and can even find fresh applications in other contexts. Another gain from reproducibility is knowledge transfer. Many times we will find an analyst that had performed an analysis at some point has moved to a new role.By having work organised with reproducibility in mind this becomes less of an issue. It is very helpful for new joiners as well as reproducible work offers what is known as paradata : an information trail that can be quite helpful when trying to understand a new codebase. Info Reproducible: A result is reproducible when the same analysis steps performed on the same dataset consistently produces the same answer. An short article in Science that can be found in this link is a very good summary of the benefits of reproducibility.","title":"Motivation and Background"},{"location":"6_reproducible/#dependencies","text":"Dependencies are all of the software components required by your project in order for it to work as intended and avoid runtime errors. One of the problems with working with open source software is that it is quite easy to fall into a trap called \u2018dependency hell\u2019. Essentially, this occurs when the software we write depends on open source packages, which depend on other open source packages, which can depend on other packages, and on, and on. All these packages may be written by many different people, and are updated at vastly different timescales. If we fail to take account of this, then we are likely to fail at the first hurdle of reproducibility, and we may find that in a year\u2019s time we are no longer able to reproduce the work that we previously did - or at least not without a lot of trouble. There are several ways we might get on top of this problem and we in this way we are going to try to explain some ways of dealing with this. Python After installing the following packages pip3 install pipreqs pip3 install pip-tools Use the following code to build a deterministic requirements.txt in the root folder of your project. That will find all requirements needed for your project to run which will be useful at a later point when dependency managment tools are going to be used. pipreqs --savepath = requirements.in && pip-compile R By typing sessionInfo () on your console you can find information about your R script's dependencies. This information is going to be really useful at a later point","title":"Dependencies"},{"location":"6_reproducible/#dependency-managment","text":"As seen above when working on a code project, one of the key concerns is dependency management . This usually means dealing with the specific packages/libraries and specific package/library versions needed for a program to run sucessfully This can be done effectively with dependency management tools: Python A tried and tested solution is using conda for this as it also installs libraries in binary format for particular machines (Windows/OSX/Linux). This is quite useful as a lot of times many packages use what is known as bootstrapping where they are compiled from source code to executable packages that can be then imported on the analyst\u2019s computer. Something that is can usually become a problem is that most computers analysts use are locked down as far as access and writing permissions are concerned or they might not include the right C++ compiler. Using conda to install the packages needed in already compiled binary format saves a lot of headaches as far as reproducibility is concerned. R Using a dependency managment tool like Renv is recommended for R.","title":"Dependency managment"},{"location":"6_reproducible/#data-is-immutable","text":"Don't ever edit your raw data, especially not manually, and especially not in Excel. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable. The code you write should move the raw data through a pipeline to your final analysis. You shouldn't have to run all of the steps every time you want to make a new figure","title":"Data is immutable"},{"location":"6_reproducible/#data-and-code-lineage","text":"Another matter that can help with reproducibility and pays dividends to be aware of is code and data provenance and lineage.","title":"Data and code lineage"},{"location":"6_reproducible/#code-lineage","text":"Different code can understandably give different results. One way to help navigate this is by having the output of your analysis to also include the git hash of the code that produced it. Python import subprocess def get_git_revision_hash (): return subprocess . check_output ([ 'git' , 'rev-parse' , 'HEAD' ]) def get_git_revision_short_hash (): return subprocess . check_output ([ 'git' , 'rev-parse' , '--short' , 'HEAD' ]) R library ( git2r ) repo <- repository ( \".\" ) print ( repository_head ( repo ))","title":"Code lineage"},{"location":"6_reproducible/#data-lineage","text":"There is also a need to do the same for data: Analysis that starts from a different point might have a different result. The solution is to include a hash of the data or any other way to help identifying the starting point of an analysis. MD5 or SH256 can help here example in command prompt: Windows certutil -hashfile <file> MD5 OSX/Linux md5 <file>","title":"Data lineage"},{"location":"6_reproducible/#keep-data-out-of-version-control","text":"You really don't want to leak your data on a public repo on Github. The way to do this is to learn use .gitignore . In this way data files should never get committed into the version control repository. You can see below hypothetical .gitignore examples for Python and R Python # keep data files in directory DATADIR out of the repo DATADIR/ # keep csv files everywhere out of the repo *.csv # Byte-compiled / optimized / DLL files __pycache__/ *.py [ cod ] * $py .class # C extensions *.so # Distribution / packaging .Python build/ develop-eggs/ R # keep data files in directory DATADIR out of the repo DATADIR/ # keep csv files everywhere out of the repo *.csv # History files .Rhistory .Rapp.history # Session Data files .RData .RDataTmp # User-specific files .Ruserdata # Example code in package build process *-Ex.R # Output files from R CMD build /*.tar.gz # Output files from R CMD check /*.Rcheck/ # RStudio files .Rproj.user/","title":"Keep data out of version control"},{"location":"6_reproducible/#keep-secrets-and-configuration-out-of-version-control","text":"You also don't want to leak your AWS secret key or database username and password on Github. Here's one way to do this: Store your secrets and config variables in a special file (for example called .env) in the root folder Add a line in .gitignore with the name of this special file. In this way this file will never get committed into the version control repository.","title":"Keep secrets and configuration out of version control"},{"location":"6_reproducible/#bold-project-gitignore-template","text":"A preliminary .gitignore template has been created and is available here . You can start with it and then edit it further to add other files/directories you want to make sure that they are not commited in a code repo. This can also be included in the cookiekcutter template mentioned in the clear layout structure document","title":"BOLD project .gitignore template"},{"location":"6_reproducible/#pre-commit-git-hooks","text":"Git hooks are scripts that Git executes before or after events such as: commit, push, and receive. Git hooks are a built-in feature - no need to download anything. Git hooks are run locally. These hook scripts are only limited by a person's imagination. Some example hook scripts include: pre-commit: Check the commit message for spelling errors. pre-receive: Enforce project coding standards. post-commit: Email/SMS team members of a new commit. post-receive: Push the code to production. We can use pre-commit git hooks by using pre-commit framework.The pre-commit framework bills itself as \"A framework for managing and maintaining multi-language pre-commit hooks.\" Under the hood, it runs on python, but you can use the framework on any project, regardless of your project's primary language. You can install this by typing pip install pre-commit on your console window Once installed, you're going to add a pre-commit configuration file to your project root named .pre-commit-config.yaml . In that config file, you will need to specify which scripts pre-commit will run when your pre-commit hook is triggered by a git commit command. There are plenty of ready made precommit hooks available here . We will use some of them now to tailor something useful for BOLD. This first example is suitable for Python code bases. A similar example with R will be provided soon. We will add certain hooks to standardise the format of the code. We will also use black to make our code fully PEP8 style guide compliant automatically. The configuration file .pre-commit-config.yaml will look something like this : repos : - repo : https://github.com/pre-commit/pre-commit-hooks rev : v3.2.0 hooks : - id : trailing-whitespace - id : end-of-file-fixer - id : check-added-large-files args : [ '--maxkb=5000' ] description : do not upload files larger than 5000kb / 5MB to avoid data being uploaded - id : mixed-line-ending args : [ '--fix=lf' ] description : Forces to replace line ending by the UNIX 'lf' character. - repo : https://github.com/psf/black rev : 20.8b1 hooks : - id : black After creating .pre-commit-config.yaml we can install it by typing pre-commit install into the root folder where the config file is. If everything has been done correctly a message saying pre-commit installed at .git/hooks/pre-commit will appear.","title":"pre-commit git hooks"},{"location":"7_QA_testing/","text":"7 QA & testing \u00b6 Motivation and Background \u00b6 Testing is a way of quality assuring our work when creating software that is important. Quality means a lot of things and for the scope of this document it will mainly mean reliability. There is a need to develop a set of tests to provide a proof that our functions work as intended. Write Tests - Any Tests! \u00b6 Starting the process of writing tests can be overwhelming, especially if you have a large code base. Further to that, as mentioned, there are many kinds of tests, and implementing all of them can seem like an impossible mountain to climb. That is why the single most important piece of guidance in this chapter is as follows: write some tests. Testing one tiny thing in a code that\u2019s thousands of lines long is infinitely better than testing nothing in a code that\u2019s thousands of lines long. You may not be able to do everything, but doing something is valuable. Unit tests should be low level,focusing on testing individual methods and functions used by your scripts. You should try to test your functions with specific inputs and expect specific outputs. Make improvements where you can, and do your best to include tests with new code you write even if it\u2019s not feasible to write tests for all the code that\u2019s already written. Run the tests \u00b6 The second most important piece of advice: run the tests. Having a beautiful, perfect test suite is no use if you rarely run it. Leaving long gaps between test runs makes it more difficult to track down what has gone wrong when a test fails because, a lot of the code will have changed. Also, if it has been weeks or months since tests have been run and they fail, it is difficult or impossible to know which results that have been obtained in the mean time are still valid, and which have to be thrown away as they could have been impacted by the bug. Info Testing frameworks: Pytest is recommended for Python while testthat is recommmened for R It is best to automate your testing as far as possible. If each test needs to be run individually then that boring painstaking process is likely to get neglected. This can be done by making use of a testing framework What to test \u00b6 Info Whenever you are tempted to type something into a print statement, write it as a test instead. There is a fine balance to writing tests. Each test that you write makes your code less likely to change inadvertently; but it also can make it harder to change your code on purpose. It\u2019s hard to give good general advice about writing tests, but you might find these points helpful: Focus on testing the external interface to your functions - if you test the internal interface, then it\u2019s harder to change the implementation in the future because as well as modifying the code, you\u2019ll also need to update all the tests. Strive to test each behaviour in one and only one test. Then if that behaviour later changes you only need to update a single test. Avoid testing simple code that you\u2019re confident will work. Instead focus your time on code that you\u2019re not sure about, is fragile, or has complicated interdependencies. That said, I often find I make the most mistakes when I falsely assume that the problem is simple and doesn\u2019t need any tests. Always write a test when you discover a bug. You may find it helpful to adopt the test-first philosophy. There you always start by writing the tests, and then write the code that makes them pass. This reflects an important problem solving strategy: start by establishing your success criteria, how you know if you\u2019ve solved the problem. Note So to summarise tests should be: Fast & Independent Repeatable (deterministic) Self-validating (no manual steps) Thorough (How much do you trust they cover everything?) The most common pattern for writing these tests is: Arrange - set up any pre-requisites for your test Act - run the code that you are testing Assert - verify that the code performed the expected action Consider how long it takes your tests to run \u00b6 Some tests, like Unit Testing only test a small piece of code and so typically are very fast. However other kinds of tests, such as System Testing which test the entire code from end to end, may take a long time to run depending on the code. As such it can be obstructive to run the entire test suite after each little bit of work. In that case it is better to run lighter weight tests such as unit tests frequently, and longer tests only once per day overnight. It is also good to scale the number of each kind of tests you have in relation to how long they take to run. You should have a lot of unit tests (or other types of tests that are fast) but much fewer tests which take a long time to run. Document the tests and how to run them \u00b6 It is important to provide documentation that describes how to run the tests, both for yourself in case you come back to a project in the future, and for anyone else that may wish to build upon or reproduce your work. This documentation should also cover subjects such as Any resources, such as test dataset files that are required Any configuration/settings adjustments needed to run the tests What software (such as testing frameworks) need to be installed Ideally, you would provide scripts to set up and configure any resources that are needed. Language specific testing tips \u00b6 Python There are many pytest plugins that add new functionality such as coverage calculation, parallelizing tests, including time information in tests (ie fail a test if it takes more than 1 min to finish!) etc R testthat is the recommended testing solution. You can find an example of how to use it in this article assertr library is very useful for data validation. It provides lots of ready made functions for checking data. assertthat package is a more advanced replacement of the base (ie included always in R) stopifnot() function","title":"7 QA and Testing"},{"location":"7_QA_testing/#7-qa-testing","text":"","title":"7 QA &amp; testing"},{"location":"7_QA_testing/#motivation-and-background","text":"Testing is a way of quality assuring our work when creating software that is important. Quality means a lot of things and for the scope of this document it will mainly mean reliability. There is a need to develop a set of tests to provide a proof that our functions work as intended.","title":"Motivation and Background"},{"location":"7_QA_testing/#write-tests-any-tests","text":"Starting the process of writing tests can be overwhelming, especially if you have a large code base. Further to that, as mentioned, there are many kinds of tests, and implementing all of them can seem like an impossible mountain to climb. That is why the single most important piece of guidance in this chapter is as follows: write some tests. Testing one tiny thing in a code that\u2019s thousands of lines long is infinitely better than testing nothing in a code that\u2019s thousands of lines long. You may not be able to do everything, but doing something is valuable. Unit tests should be low level,focusing on testing individual methods and functions used by your scripts. You should try to test your functions with specific inputs and expect specific outputs. Make improvements where you can, and do your best to include tests with new code you write even if it\u2019s not feasible to write tests for all the code that\u2019s already written.","title":"Write Tests - Any Tests!"},{"location":"7_QA_testing/#run-the-tests","text":"The second most important piece of advice: run the tests. Having a beautiful, perfect test suite is no use if you rarely run it. Leaving long gaps between test runs makes it more difficult to track down what has gone wrong when a test fails because, a lot of the code will have changed. Also, if it has been weeks or months since tests have been run and they fail, it is difficult or impossible to know which results that have been obtained in the mean time are still valid, and which have to be thrown away as they could have been impacted by the bug. Info Testing frameworks: Pytest is recommended for Python while testthat is recommmened for R It is best to automate your testing as far as possible. If each test needs to be run individually then that boring painstaking process is likely to get neglected. This can be done by making use of a testing framework","title":"Run the tests"},{"location":"7_QA_testing/#what-to-test","text":"Info Whenever you are tempted to type something into a print statement, write it as a test instead. There is a fine balance to writing tests. Each test that you write makes your code less likely to change inadvertently; but it also can make it harder to change your code on purpose. It\u2019s hard to give good general advice about writing tests, but you might find these points helpful: Focus on testing the external interface to your functions - if you test the internal interface, then it\u2019s harder to change the implementation in the future because as well as modifying the code, you\u2019ll also need to update all the tests. Strive to test each behaviour in one and only one test. Then if that behaviour later changes you only need to update a single test. Avoid testing simple code that you\u2019re confident will work. Instead focus your time on code that you\u2019re not sure about, is fragile, or has complicated interdependencies. That said, I often find I make the most mistakes when I falsely assume that the problem is simple and doesn\u2019t need any tests. Always write a test when you discover a bug. You may find it helpful to adopt the test-first philosophy. There you always start by writing the tests, and then write the code that makes them pass. This reflects an important problem solving strategy: start by establishing your success criteria, how you know if you\u2019ve solved the problem. Note So to summarise tests should be: Fast & Independent Repeatable (deterministic) Self-validating (no manual steps) Thorough (How much do you trust they cover everything?) The most common pattern for writing these tests is: Arrange - set up any pre-requisites for your test Act - run the code that you are testing Assert - verify that the code performed the expected action","title":"What to test"},{"location":"7_QA_testing/#consider-how-long-it-takes-your-tests-to-run","text":"Some tests, like Unit Testing only test a small piece of code and so typically are very fast. However other kinds of tests, such as System Testing which test the entire code from end to end, may take a long time to run depending on the code. As such it can be obstructive to run the entire test suite after each little bit of work. In that case it is better to run lighter weight tests such as unit tests frequently, and longer tests only once per day overnight. It is also good to scale the number of each kind of tests you have in relation to how long they take to run. You should have a lot of unit tests (or other types of tests that are fast) but much fewer tests which take a long time to run.","title":"Consider how long it takes your tests to run"},{"location":"7_QA_testing/#document-the-tests-and-how-to-run-them","text":"It is important to provide documentation that describes how to run the tests, both for yourself in case you come back to a project in the future, and for anyone else that may wish to build upon or reproduce your work. This documentation should also cover subjects such as Any resources, such as test dataset files that are required Any configuration/settings adjustments needed to run the tests What software (such as testing frameworks) need to be installed Ideally, you would provide scripts to set up and configure any resources that are needed.","title":"Document the tests and how to run them"},{"location":"7_QA_testing/#language-specific-testing-tips","text":"Python There are many pytest plugins that add new functionality such as coverage calculation, parallelizing tests, including time information in tests (ie fail a test if it takes more than 1 min to finish!) etc R testthat is the recommended testing solution. You can find an example of how to use it in this article assertr library is very useful for data validation. It provides lots of ready made functions for checking data. assertthat package is a more advanced replacement of the base (ie included always in R) stopifnot() function","title":"Language specific testing tips"},{"location":"Code_of_Conduct/","text":"Contributor Covenant Code of Conduct \u00b6 GOV.UK code of conduct \u00b6 Our pledge \u00b6 In order to create an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of: age body size disability ethnicity gender identity and expression level of experience nationality personal appearance race religion sexual identity and orientation Our standards \u00b6 You can contribute to creating a positive environment in many ways. For example you can: use welcoming and inclusive language be respectful of differing viewpoints and experiences accept constructive criticism gracefully focus on what is best for the community show empathy towards other community members You should not: use sexualised language or imagery make unwelcome sexual advances troll, and make insulting or derogatory comments make personal or political attacks harass others, in public or private publish others' private information, such as a physical or electronic address, without explicit permission engage in any other conduct which could reasonably be considered inappropriate in a professional setting Our responsibilities \u00b6 As project maintainers, we are responsible for clarifying the standards of acceptable behaviour and we are expected to take appropriate and fair corrective action in response to any instances of unacceptable behaviour. We have the right and responsibility to remove, edit, or reject: comments commits code wiki edits issues other contributions that are not aligned to this code of conduct We also reserve the right to temporarily or permanently ban any contributor for other behaviours we deem inappropriate, threatening, offensive, or harmful. Scope \u00b6 This code of conduct applies whenever you are representing the project or community. For example you may be: working in a project space online or in the public using an official project email address posting via an official social media account participating in an online or offline event Project maintainers may further define and clarify representation of a project. Enforcement \u00b6 You should report any instances of abusive, harassing, or otherwise unacceptable behaviour to the project team at appropriate contact details The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain the anonymity of the reporter of an incident. We may post further details of specific enforcement policies separately. Project maintainers who do not follow or enforce this code of conduct in good faith may face temporary or permanent consequences. These will be determined by members of the project's leadership. Attribution \u00b6 This code of conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4","title":"Contributor Covenant Code of Conduct"},{"location":"Code_of_Conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"Code_of_Conduct/#govuk-code-of-conduct","text":"","title":"GOV.UK code of conduct"},{"location":"Code_of_Conduct/#our-pledge","text":"In order to create an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of: age body size disability ethnicity gender identity and expression level of experience nationality personal appearance race religion sexual identity and orientation","title":"Our pledge"},{"location":"Code_of_Conduct/#our-standards","text":"You can contribute to creating a positive environment in many ways. For example you can: use welcoming and inclusive language be respectful of differing viewpoints and experiences accept constructive criticism gracefully focus on what is best for the community show empathy towards other community members You should not: use sexualised language or imagery make unwelcome sexual advances troll, and make insulting or derogatory comments make personal or political attacks harass others, in public or private publish others' private information, such as a physical or electronic address, without explicit permission engage in any other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our standards"},{"location":"Code_of_Conduct/#our-responsibilities","text":"As project maintainers, we are responsible for clarifying the standards of acceptable behaviour and we are expected to take appropriate and fair corrective action in response to any instances of unacceptable behaviour. We have the right and responsibility to remove, edit, or reject: comments commits code wiki edits issues other contributions that are not aligned to this code of conduct We also reserve the right to temporarily or permanently ban any contributor for other behaviours we deem inappropriate, threatening, offensive, or harmful.","title":"Our responsibilities"},{"location":"Code_of_Conduct/#scope","text":"This code of conduct applies whenever you are representing the project or community. For example you may be: working in a project space online or in the public using an official project email address posting via an official social media account participating in an online or offline event Project maintainers may further define and clarify representation of a project.","title":"Scope"},{"location":"Code_of_Conduct/#enforcement","text":"You should report any instances of abusive, harassing, or otherwise unacceptable behaviour to the project team at appropriate contact details The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain the anonymity of the reporter of an incident. We may post further details of specific enforcement policies separately. Project maintainers who do not follow or enforce this code of conduct in good faith may face temporary or permanent consequences. These will be determined by members of the project's leadership.","title":"Enforcement"},{"location":"Code_of_Conduct/#attribution","text":"This code of conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4","title":"Attribution"},{"location":"Contributing/","text":"The BOLD Best Practice project is opinionated, but not afraid to be wrong. Best practices change, tools evolve, and lessons are learned. The goal of this project is to make it easier to start, structure, and share an analysis. Pull requests and filing issues is encouraged. We'd love to hear what works for you, and what doesn't.","title":"Contributing"},{"location":"orgs/DHSC/","text":"","title":"DHSC"},{"location":"orgs/MOJ/","text":"","title":"MOJ"},{"location":"orgs/ONS/","text":"","title":"ONS"}]}